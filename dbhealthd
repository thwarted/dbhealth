#!/usr/bin/python
# vim: set fileencoding=utf8: ts=4 expandtab:

from __future__ import with_statement

import sys
import os
import time
import struct
import mmap
import os
import re
import posix
import signal
import socket
import string
import _mysql
import _mysql_exceptions
import select
import cPickle as pickle
import traceback
import ConfigParser
from setproctitle import setproctitle
from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer

class dbstate:
    servers = []
    ignore = []

lockedup_after = 600

config_file = './dbhealthd.cnf'

dbhealth_port = 3591
maxslots = 200 # total number of databases we support being queried at once
maxslaves = 15
pidfilepath = "/tmp/dbhealthd.pid"

storage_type = 'simple'
running = True


_empty_slaves = str(socket.inet_aton('0.0.0.0')) * maxslaves

if storage_type == 'pickle':
    sample_struct = {'node':"x" * 30, # svname (service name) in haproxy
                 'hostname':"x" * 30, # what the machine thinks its hostname is
                   'pxname':"x" * 30, # haproxy's proxy name
                  'version':"x" * 50,
                       'ip':socket.inet_aton('255.255.255.255'),
                   'master':socket.inet_aton('255.255.255.255'),
                   'slaves':str(socket.inet_aton('255.255.255.255')) * maxslaves,
                     'port':(2 ** 16),
                     'time':int(time.time()),
                      'act':True,
                  'pscount':long(2 ** 31),
                   'maxcon':long(2 ** 31),
                       'ro':True,
                      'pid':long(2 ** 31),
                  'queries':long(2 ** 31),
                      'qps':long(2 ** 31),
                   'uptime':long(2 ** 31),
                    'delay':long(2 ** 31),
                     'repl':long(2 ** 31)}
    stored_keys = sample_struct.keys()
    sample_struct_s = pickle.dumps(sample_struct)
    slotsize = len(sample_struct_s) + 10
    slotformat = str(slotsize) + 's'
    slotsize = struct.calcsize(slotformat)
else:
    stored_keys = ('node', 'hostname', 'pxname', 'time', 'pid', 'queries', 'uptime', 'pscount', 'maxcon', 'delay', 'repl', 'act', 'ro', 'port', 'master', 'ip', 'version', 'qps', 'slaves')
    slotformat =    '30s'       '30s'     '30s'     'q'    'q'        'q'       'q'        'q'       'q'      'i'     'i'    'c'   'c'     'i'      '4s'  '4s'      '50s'  'q'
    slotformat += str(maxslaves * 4) + 's'
    slotsize = struct.calcsize(slotformat)

shmmaxsize = slotsize * maxslots
dead_children = []
pending_producers = []
sleeptime = 1

#one_producer = (slot, node-name, child-pid)
# producers are kept in sorted order by slot
producers = []

pending_new_producer = False


def store_simple(slot, **a):
    global shared
    global slotsize
    global slotformat
    global stored_keys

    c = []
    for k in stored_keys:
        c.append(a[k])
    struct.pack_into(slotformat, shared, slot*slotsize, *c)

def retrieve_simple(slot):
    global shared
    global slotsize
    global slotformat
    global stored_keys

    b = [x for x in struct.unpack_from(slotformat, shared, slot*slotsize)]
    c = {}
    for k in stored_keys:
        c[k] = b.pop(0)
        if k not in ('ip','slaves') and isinstance(c[k], str):
            c[k] = c[k].rstrip("\0")
    return c

def cleanup_state_dict(**a):
    # each slot stores
    # node, ip, port, when, delay_slave, delay_repl, node_active, read_only
    global maxslaves

    if not a.has_key('node'):
        raise KeyError('missing node key')

    if a.has_key('ip'):
        a['ip'] = socket.inet_aton(a['ip'])
    else:
        a['ip'] = socket.inet_aton('0.0.0.0')

    if not a.has_key('hostname'):
        a['hostname'] = ''

    if not a.has_key('master'):
        a['master'] = socket.inet_aton('0.0.0.0')
    elif len(a['master']) != 4:
        a['master'] = socket.inet_aton(a['master'])

    if not a.has_key('port'):
        a['port'] = 3306

    if not a.has_key('pxname'):
        a['pxname'] = ''

    if not a.has_key('pscount'):
        a['pscount'] = 0

    if not a.has_key('maxcon'):
        a['maxcon'] = 0

    if not a.has_key('time'):
        a['time'] = int(time.time())

    if not a.has_key('queries'):
        a['queries'] = -1

    if not a.has_key('qps'):
        a['qps'] = -1

    if not a.has_key('uptime'):
        a['uptime'] = -1

    if not a.has_key('delay'):
        a['delay'] = -1

    if not a.has_key('repl'):
        a['repl'] = -1

    if a.has_key('version'):
        a['version'] = a['version'][0:50]
    else:
        a['version'] = ''

    # normalize active to !, 1 or 0
    if not a.has_key('act'):
        a['act'] = '!'
    elif a['act'] != '0':
        a['act'] = '1'
    else:
        a['act'] = '0'

    # normalize readonly to !, 1 or 0
    if not a.has_key('ro'):
        a['ro'] = '!'
    elif a['ro'] != '0':
        a['ro'] = '1'
    else:
        a['ro'] = '0'

    if not a.has_key('slaves'):
        a['slaves'] = _empty_slaves
    else:
        x = ''.join(socket.inet_aton(x) for x in a['slaves'])
        x += _empty_slaves
        a['slaves'] = x[:maxslaves*4]

    a['pid'] = os.getpid()

    b = {}
    for k in stored_keys:
        b[k] = a[k]

    return b

def store_pickle(slot, **a):
    global shared
    global slotsize
    global slotformat
    global stored_keys

    struct.pack_into(slotformat, shared, slot*slotsize, pickle.dumps(a))

def retrieve_pickle(slot):
    global shared
    global slotsize

    (a,) = struct.unpack_from(slotformat, shared, slot*slotsize)
    try:
        a = pickle.loads(a)
    except:
        a = {}
    return a

def store(slot, **a):
    a = cleanup_state_dict(**a)
    if storage_type == "pickle":
        return store_pickle(slot, **a)
    else:
        return store_simple(slot, **a)

def retrieve(slot):
    if storage_type == "pickle":
        a = retrieve_pickle(slot)
    else:
        a = retrieve_simple(slot)

    if 'time' in a:
        a['dataage'] = int(time.time()) - a['time']
    a['slot'] = slot
    if 'master' in a and len(a['master']) == 4:
        a['master'] = socket.inet_ntoa(a['master'])
    else:
        a['master'] = '-'

    a['ip'] = socket.inet_ntoa(a['ip'])
    a['slaves'] = tuple(socket.inet_ntoa(a['slaves'][x:x+4]) for x in range(0, len(a['slaves']), 4) if a['slaves'][x:x+4] != '\x00\x00\x00\x00')
    a['slavecount'] = len(a['slaves'])

    return a

def retrieve_node(node):
    global producers

    for x in producers:
        s, n, p = x
        if node == n:
            return retrieve(s)
    return {}

def clear_slot(slot):
    #sys.stderr.write("clearing slot %d\n" % (slot,))
    store(slot, node="")


def foreach_slot_raw():
    for slot in range(0,maxslots):
        yield retrieve(slot)


def foreach_slot():
    for slot in foreach_slot_raw():
        if slot:
            pid = slot['pid']
            try:
                if os.kill(pid, 0):
                    yield slot
            except OSError:
                pass

def maybe_abstract_int(i, maxlen=5):
    i = int(i)
    s = str(i)
    if len(str(i)) > maxlen:
        s = '%0.2fk' % (float(i) / 1000.0,)
    if len(s) > maxlen:
        s = '%0.2fm' % (float(i) / 1000000.0,)
    if len(s) > maxlen:
        s = '%0.2fg' % (float(i) / 1000000000.0,)
    if len(s) > maxlen:
        s = 'inf'
    return s

def setup_shared_area():
    devzero = posix.open("/dev/zero", posix.O_RDWR)
    shared = mmap.mmap(devzero, shmmaxsize, mmap.MAP_SHARED, mmap.PROT_READ | mmap.PROT_WRITE)
    #sys.stderr.write(repr((shmmaxsize, shared))+"\n")
    return shared


def find_free_slot(producers):
    c = 0
    for p in producers:
        if p[0] != c:
            return c
        c = c + 1
    assert c < maxslots, "out of slots"
    return c


def spawn_new_producer(dbinfo, wfile=None):
    global producers
    global pending_new_producer
    global server

    if 'name' not in dbinfo:
        # TODO issue a warning here, bad format of dbinfo, needs at least 'name' key
        return
    node = dbinfo['name']

    try:
        dbinfo['ip'] = socket.gethostbyname(node)
    except socket.gaierror:
        # TODO issue a warning here, no DNS entry
        # TODO or remove the entry all together
        # TODO if a DNS entry is added, the dbstate file can be touched to have it picked up
        return

    if 'pxname' not in dbinfo:
        dbinfo['pxname'] = ''

    slot = find_free_slot(producers)
    store(slot, node=node)
    childpid = os.fork()
    if not childpid:
        signal.signal(signal.SIGCHLD, signal.SIG_DFL)
        signal.signal(signal.SIGTERM, signal.SIG_DFL)
        signal.signal(signal.SIGUSR1, signal.SIG_DFL)
        time.sleep(0.1) # kludge. help avoid a race condition (FIXME: what was the race condition?!)
        if wfile is not None:
            os.close(wfile.fileno()) # we don't want to shutdown(2) these sockets, just close them in the child
        os.close(server.socket.fileno())
        setproctitle('dbhealthd collector: %s @ %s' % (node, dbinfo['ip']))
        producer(slot, dbinfo)
        sys.exit(0)

    producers.append((slot, node, childpid))
    producers.sort(lambda a,b: cmp(a[0], b[0]))
    #sys.stderr.write("    created new producer in slot(%05d)\n" % (slot,))
    #sys.stderr.write(repr(producers)+"\n")
    pending_new_producer = False

def producer(slot, dbinfo):
    global shared
    global server
    global sleeptime

    def queries_total(db, last_query):
        delay_query = "show global status like 'Questions'"
        last_query[0] = delay_query
        db.query(delay_query)
        rs = db.store_result()
        assert rs.num_rows() == 1, "returned rows is not 1"
        x = rs.fetch_row(1, 0)[0]
        _, queries = x
        return long(queries)

    def processlistcount(db, last_query):
        delay_query = "select count(1) as pscount, @@max_connections as max_connections from information_schema.processlist"
        last_query[0] = delay_query
        db.query(delay_query)
        rs = db.store_result()
        assert rs.num_rows() == 1, "returned rows is not 1"
        x = rs.fetch_row(1, 0)[0]
        (pscount, maxcon) = x
        return (long(pscount), long(maxcon))

    def server_uptime(db, last_query):
        delay_query = "show global status like 'Uptime'"
        last_query[0] = delay_query
        db.query(delay_query)
        rs = db.store_result()
        assert rs.num_rows() == 1, "returned rows is not 1"
        x = rs.fetch_row(1, 0)[0]
        _, uptime = x
        return long(uptime)

    def get_database_state(db, last_query):
        delay_query = 'select @@hostname, @@version, @@read_only, unix_timestamp(), -1'
        last_query[0] = delay_query
        db.query(delay_query)
        rs = db.store_result()
        assert rs.num_rows() == 1, "returned rows is not 1"
        x = rs.fetch_row(1, 0)[0]
        (dbhostname, version, is_read_only, current_time, repl_heartbeat) = x
        if repl_heartbeat is None:
            repl_heartbeat = -1
        else:
            repl_heartbeat = int(repl_heartbeat)
        return (dbhostname, version, is_read_only, int(current_time), repl_heartbeat)

    def is_node_active(db, last_query):
        last_query[0] = "show databases like '_disabled_by_updown'"
        db.query(last_query[0])
        rs = db.store_result()
        node_active = '1'
        if rs.num_rows() == 1:
            node_active = '0'
        return node_active

    def replicating_slaves(db, last_query):
        last_query[0] = "select trim(trailing concat(':',substring_index(host, ':', -1)) from host) as slave " + \
                        "  from information_schema.processlist where command in ('Binlog Dump', 'Binlog Dump GTID')"
        db.query(last_query[0])
        rs = db.store_result()
        slaves = []
        for row in rs.fetch_row(0):
            slaves.append(row[0])
        return tuple(slaves)

    def slave_status(db, last_query):
        last_query[0] = "show slave status"
        db.query(last_query[0])
        rs = db.store_result()
        sl_data = rs.fetch_row(1, 1)
        if len(sl_data):
            master_host = sl_data[0]['Master_Host']
            slave_delay = sl_data[0]['Seconds_Behind_Master']
            if slave_delay is None:
                slave_delay = -1
            else:
                slave_delay = int(slave_delay)
        else:
            slave_delay = -1
            master_host = '0.0.0.0'
        return (slave_delay, master_host)

    sleeptime = 1
    def reset_sleeptime(sig, context):
        global sleeptime
        sys.stderr.write("  reset sleeptime\n")
        sleeptime = 1
    signal.signal(signal.SIGHUP, reset_sleeptime)

    db = None
    name = dbinfo['name']
    ip = dbinfo['ip']
    shortname = ip2shortname(ip)
    last_query = [''] # this is a list so the above functions can modify it in place

    store(slot, node=name, hostname=shortname, ip=ip)

    last_querycount = 0
    last_state_time = 0
    while True:
        try:
            if os.getppid() == 1:
                sys.stderr.write("%s: parent seems to have died, exiting\n" % (name,))
                break

            time.sleep(sleeptime)
            if not db:
                try:
                    #sys.stderr.write("attempting to connect to %s\n" % (name,))
                    db = _mysql.connect(read_default_file=config_file, read_default_group='mysql-connect', host=dbinfo['ip'])
                except:
                    db = None
                if not db:
                    sleeptime = sleeptime * 2
                    if sleeptime > 120:
                        sleeptime = 120
                    setproctitle('dbhealthd collector: %s @ %s (unable to connect, sleeping %d, HUP to reset)' % (shortname, ip, sleeptime))
                    store(slot, node=name, hostname=shortname, ip=ip)
                    continue

            sleeptime = 1 # if we made it to here, we're (finally) connected, so reset sleep time

            (dbhostname, version, is_read_only, current_time, repl_heartbeat) = get_database_state(db, last_query)
            (slave_delay, master_host) = slave_status(db, last_query)
            node_active = is_node_active(db, last_query)
            queries = queries_total(db, last_query)
            uptime = server_uptime(db, last_query)
            pscount, maxcon = processlistcount(db, last_query)
            slaves = replicating_slaves(db, last_query)

            qps = -2
            if last_state_time:
                if current_time - last_state_time > 0:
                    qps = (queries - last_querycount) / (current_time - last_state_time)
            last_state_time = current_time
            last_querycount = queries

            store(slot, node=name, hostname=dbhostname, delay=slave_delay, repl=repl_heartbeat, act=node_active, ro=is_read_only,
                        master=master_host, version=version, pxname=dbinfo['pxname'], queries=queries, uptime=uptime, pscount=pscount,
                        maxcon=maxcon, qps=qps, slaves=slaves, ip=ip)

            setproctitle('dbhealthd collector: %s @ %s (delay=%d)' % (dbhostname, ip, repl_heartbeat))
        except _mysql_exceptions.OperationalError, e:
            sys.stderr.write("OperationalError for %s: %r\n" % (name, e))
            sys.stderr.write(last_query[0] + "\n")
            traceback.print_exc(file=sys.stderr)
            store(slot, node=name)
            db = None
        except Exception, e:
            sys.stderr.write("Exception for %s: %r\n" % (name, e))
            sys.stderr.write(last_query[0] + "\n")
            traceback.print_exc(file=sys.stderr)
            store(slot, node=name)

    sys.exit(0)

def child_reaper(sig, context):
    global dead_children
    global producers
    # try to reap any and all dead children
    try:
        while True:
            (pid, _, _) = os.wait3(os.WNOHANG)
            if pid:
                #sys.stderr.write("    found dead child %d\n" % (pid,))
                dead_children.append(pid)
            else:
                break
    except OSError, e:
        # os.wait3 returned ECHILD, no children to wait on
        # this could happen since a single SIGCHILD signal could end up reaping
        # multiple children
        pass
    except Exception, e:
        sys.stderr.write("error in child_reaper signal handler: %r\n" % (e,))

def flag_new_producer(sig, context):
    global pending_new_producer
    #sys.stderr.write("flagging for new producer\n")
    pending_new_producer = True

def ip2shortname(ip, cache={}):
    name = None
    if cache.has_key(ip):
        name, age = cache[ip]
        if age > time.time() - 600:
            return name
    try:
        name = socket.gethostbyaddr(ip)[0].split('.')[0]
    except:
        name = ''
    cache[ip] = (name, time.time())
    return name

class MyHandler(BaseHTTPRequestHandler):
    totalrequests = 0
    lastindent = 0
    lastpxname = ''

    def log_request(self, *args, **kwargs):
        pass

    def GET_report(self, splitpath):
        global producers

        defaultfields = {'node':   '-30s',
                         'hostname':'-30s',
                         'masterstr':'-20s',
                         'ip':      '-15s',
                         'master':  '-15s',
                         'pxname':  '10s',
                         'slot':    '4s',
                         'dataage': '7s',
                         'pscount': '12s',
                         'act':     '3s',
                         'ro':      '2s',
                         'delay':   '5s',
                         'repl':    '5s',
                         'queries': '13s',
                         'qps':     '10s',
                         'slavecount': '5s',
                         'version': 's'}

        if len(splitpath) == 0:
            splitpath = ('hostname', 'masterstr', 'pxname', 'dataage', 'pscount', 'act', 'ro', 'delay', 'repl', 'queries', 'qps', 'ip', 'slot', 'version')

        #if not splitpath:
        #    splitpath = defaultfields.keys()


        def print_tree(plist, masterstr='', indent=0, printedheader=[], fields=None, alt=[0]):
            """ print the replication tree rooted at masterstr (the hostname of the master, not the IP)
            the root master of a cluster should be the only machine in rw mode

            if master is None, print plist itself
            if master is the empty string, print the tree rooted at nodes that are in rw mode (the master for a cluster)
            if master is not the empty string, print the tree that is rooted at that node
            """

            if masterstr is None:
                matches = plist
            elif masterstr == '':
                matches = []
                todel = []
                for i in range(len(plist)):
                    if plist[i]['ro'] == '0':
                        matches.append(plist[i])
                        todel.append(i)
                todel.sort()
                todel.reverse()
                for i in todel:
                    del plist[i]
            else:
                matches = []
                todel = []
                for i in range(len(plist)):
                    if plist[i]['masterstr'] == masterstr:
                        matches.append(plist[i])
                        todel.append(i)
                todel.sort()
                todel.reverse()
                for i in todel:
                    del plist[i]


            # format a format string for the list of fields asked for using the format strings in defaultfields
            formatstring = " ".join(["%("+f+")"+defaultfields[f] for f in fields])

            #lastmatchindex = len(matches)
            for i in range(len(matches)):
                a = matches[i]
                x = dict(**a) # copy it because we're going to munge it
                if not printedheader:
                    # format a format string for the list of fields asked for using the format strings in defaultfields
                    p = " ".join(["%("+f+")"+re.sub(r'\..+$', 's', defaultfields[f]) for f in fields])
                    # print the header names
                    self.wfile.write(p % dict([(d,d) for d in x.keys()]))
                    self.wfile.write("\n")
                    # print the header separator
                    self.wfile.write(p % dict([(d,'-'*len(d)) for d in x.keys()]))
                    self.wfile.write("\n")
                    printedheader.append(1)
                else:
                    # we only want to print a separating line before the current line if we're
                    # in the body of the data, not right after the header is printed
                    if (self.lastindent and not indent) or (indent == 0 and self.lastpxname != a['pxname']):
                        self.wfile.write("\n")
                    pass
                if x['hostname'] != ip2shortname(x['ip']):
                   x['hostname'] += ' ('+ ip2shortname(x['ip']) + ')'

                if x['maxcon']:
                    x['pscount'] = '%d/%3d%%' % (x['pscount'], int(float(x['pscount']) / float(x['maxcon']) * 100.0))
                else:
                    x['pscount'] = '%d/?   ' % (x['pscount'],)

                x['queries'] = maybe_abstract_int(x['queries'], maxlen=13)
                x['delay'] = maybe_abstract_int(x['delay'], maxlen=5)
                x['repl'] = maybe_abstract_int(x['repl'], maxlen=5)

                x['hostname'] = (" " * indent) + x['hostname']

                x = formatstring % x
                self.wfile.write(x + "\n")
                self.lastindent = indent
                self.lastpxname = a['pxname']
                if masterstr is not None:
                    print_tree(plist, masterstr=a['hostname'], indent=indent+1, fields=fields)

        self.send_response(200)
        self.send_header('Content-type', 'text/plain')
        self.end_headers()

        if len(splitpath) >= 1 and splitpath[0] == 'help':
            self.wfile.write("/sorteddelay/xxx/yyy/zzz/...\n")
            self.wfile.write("where /xxx/yyy/zzz/... is a series of:\n")
            for f in defaultfields.keys():
                self.wfile.write(" - " + f + "\n")
            self.wfile.write("\n")
            return

        plist = []
        for x in producers:
            (s, n, p) = x
            a = retrieve(s)
            if not a['node']: # empty slot
                continue
            a['masterstr'] = ip2shortname(a['master'])
            plist.append(a)

        self.lastindent = 0
        self.lastpxname = ''
        print_tree(plist, masterstr='', fields=splitpath) # print the tree
        print_tree(plist, masterstr=None, fields=splitpath) # print any that are left, should sort this based on pxname or master so they are grouped better
        self.wfile.write("\n\n")

    def GET_health(self, splitpath):
        message = "missing node name"
        if len(splitpath) == 2:
            node = retrieve_node(splitpath[1])
            message = "unknown node"
            if node:
                if dbstate.health(node):
                    self.send_response(200)
                    self.send_header('Content-type', 'text/plain')
                    self.end_headers()
                    self.wfile.write("OK\n")
                    return
                else:
                    self.send_error(503, "failed health check")
                    return
        self.send_error(404, message)

    def GET_status(self):
        # FIXME this is non-standard output format, just return 200 and the output UP
        self.send_response(200)
        self.send_header('Content-type', 'text/plain')
        self.end_headers()
        self.wfile.write("UP\n")

    def GET_status_version(self):
        global versionlabel
        self.send_response(200)
        self.send_header('Content-type', 'text/plain')
        self.end_headers()
        self.wfile.write(versionlabel + "\n")

    def GET_addserver(self, path):
        node = path[0]
        try:
            ip = socket.gethostbyname(node)
            if not dbstate_servers_has(ip):
                dbstate.servers.append({'name':ip, 'pxname': '-'})
        except socket.gaierror:
            pass
        self.send_response(200)
        self.send_header('Content-type', 'text/plain')
        self.end_headers()
        self.wfile.write(node + " added\n")

    def do_GET(self):
        self.totalrequests = self.totalrequests + 1
        #sys.stderr.write("%d total requests\n" % (self.totalrequests,))

        try:
            # TODO convert this URL dispatching to something a little more
            #      consistent.  The called functions themselves will have
            #      to have their arguments normalized

            if self.path.endswith(".ico"):
                self.send_error(404, "File not found")
                return

            if self.path == '/status':
                self.GET_status()
                return

            if self.path == '/status/version':
                self.GET_status_version()
                return

            splitpath = filter(lambda x: x, self.path.split("/"))
            assert len(splitpath) > 0

            if splitpath[0] == 'addserver':
                self.GET_addserver(splitpath[1:])
                return

            if splitpath[0] == 'report':
                self.GET_report(splitpath[1:])
                return

            if splitpath[0] == 'sorteddelay':
                self.GET_report(splitpath[1:])
                return

            if splitpath[0] == "slavedelay":
                self.GET_slavedelay(splitpath[1:])
                return

            if splitpath[0] == "health":
                self.GET_health(splitpath)
                return

        except Exception, e:
            sys.stderr.write(repr(e)+"\n")
            traceback.print_exc(file=sys.stderr)

        self.send_error(404, "File Not Found")


def normalize_dbstate():
    global dbstate

    # a name may have been given in the server seed list, so normalize
    # those names to IP addresses
    for i in xrange(len(dbstate.servers)):
        dbstate.servers[i]['name'] = socket.gethostbyname(dbstate.servers[i]['name'])

dbstate_stat_mtime = 0

def reload_dbstate_config():
    """ attempt to reload the dbstate module
    return boolean indicating if we reloaded it or not
    """
    global dbstate
    global dbstate_stat_mtime
    global config_file

    try:
        o = os.stat(config_file)
        if not len(dbstate.servers) or (dbstate_stat_mtime > 0 and o.st_mtime != dbstate_stat_mtime):
            c = ConfigParser.RawConfigParser()
            c.read(config_file)
            dbstate.servers = []
            for pxname, servername in c.items('seedservers'):
                pxname = re.split(r'\d+.*$', pxname)[0]
                dbstate.servers.append({'name':servername, 'pxname': pxname})
            for x, servername in c.items('ignore'):
                ip = socket.gethostbyname(servername)
                name = ip2shortname(ip)
                dbstate.ignore.extend([ip, name])
            sys.stderr.write("reloaded %s\n" % (config_file,))
            normalize_dbstate()
            dbstate_stat_mtime = o.st_mtime
            return True
        else:
            dbstate_stat_mtime = o.st_mtime
    except SyntaxError, e:
        traceback.print_exc(file=sys.stderr)
    return False

def handle_http_request(server):
    global running
    try:
        server.handle_request()
    except KeyboardInterrupt:
        server.socket.close()
        return False
    except select.error:
        # since SIGCHILD could interrupt us
        if not running:
           return False
    except Exception, e:
        sys.stderr.write(repr(e)+"\n")
        return False
    return True


def create_pending_producers(pending_producers):
    if pending_producers:
        for p in pending_producers:
            spawn_new_producer(p)
        pending_producers = []
    return pending_producers


def clean_up_dead_children(dead_children):
    global producers
    if dead_children:
        cleanedup = []
        for slot, node_name, childpid in producers:
            if childpid in dead_children:
                cleanedup.append(node_name)
                clear_slot(slot)
        c = len(producers)
        producers = filter(lambda x: x[2] not in dead_children, producers)
        #sys.stderr.write("cleaned up %d children (%r)\n" % (c - len(producers),cleanedup))
        #sys.stderr.write(repr(producers)+"\n")
        dead_children = []
    return dead_children


def dump_slots(sig, context):
    slot = 0
    for s in foreach_slot_raw():
        slot = slot + 1
        sys.stderr.write("%d: %r\n" % (slot, s))

def kill_all_children(producerslist, signum=signal.SIGTERM):
    for slot, node_name, childpid in producerslist:
        #sys.stderr.write("killing %d (slot %d, node %s)\n" % (childpid, slot, node_name))
        try:
            os.kill(childpid, signum)
            clear_slot(slot)
        except:
            pass
        # will be reaped by the SIGCHILD signal handler

def spawn_all_children(serverlist):
    for dbinfo in serverlist:
        spawn_new_producer(dbinfo)

def find_non_running_producers(serverlist, producerslist, pending_producers):
    active_producers = tuple(x[1] for x in producerslist) # element 1 is the name of the node the producer is looking at

    for dbinfo in serverlist:
        # TODO issue warning here, server list isn't in the correct format
        # although we would have already done this when we first iterate
        # over these when the dbstate module is loaded
        if 'name' in dbinfo:
            if dbinfo['name'] not in active_producers:
                pending_producers.append(dbinfo)

def find_lockedup_producers(producerslist):
    global lockedup_after

    for slot, node_name, childpid in producerslist:
        a = retrieve(slot)
        if a['dataage'] > lockedup_after:
            try:
                os.kill(childpid, signal.SIGKILL)
                clear_slot(slot)
            except:
                pass

def cleanexit(sig, context):
    global running
    running = False

def find_version_string():
    # CEP93 version determination: read the file "version" created by the deploy process
    vfile = os.path.join(os.path.dirname(sys.argv[0]), 'version')
    if os.path.isfile(vfile):
        with file(vfile, "r") as f:
            vstr = f.readline()
        return vstr.strip()
    return "unknown-%s-%d" % (os.uname()[1], os.getpid())

def dbstate_servers_has(ip):
    for srv in dbstate.servers:
        if srv['name'] == ip:
            return True
    return False

def find_new_servers():
    global producers

    for x in producers:
        (s, n, p) = x
        a = retrieve(s)

        if not a['node']: # empty slot
            continue

        # do we already know about this node's master?
        try:
            ip = socket.gethostbyname(a['master'])
            if ip not in dbstate.ignore and not dbstate_servers_has(ip):
                dbstate.servers.append({'name':ip, 'pxname': '-'})
        except socket.gaierror:
            pass

        # do we already know about all the slaves of this node?
        for ip in a['slaves']:
            try:
                ip = socket.gethostbyname(ip)
                if ip not in dbstate.ignore and not dbstate_servers_has(ip):
                    dbstate.servers.append({'name':ip, 'pxname': '-'})
            except socket.gaierror:
                pass


if __name__ == "__main__":
    versionlabel = find_version_string()

    sys.stderr.write("slot size is %d\n" % (slotsize,))
    sys.stderr.write("shmsize is %d\n" % (shmmaxsize,))

    shared = setup_shared_area()

    with file(pidfilepath, "w") as pidfile:
        pidfile.write(str(os.getpid())+"\n")

    signal.signal(signal.SIGCHLD, child_reaper)
    server = HTTPServer(('', dbhealth_port), MyHandler)
    server.socket.settimeout(1)
    setproctitle('dbhealthd master: http://%s:%d/help' % (os.uname()[1], dbhealth_port))

    signal.signal(signal.SIGTERM, cleanexit)
    signal.signal(signal.SIGUSR1, dump_slots)

    normalize_dbstate()
    spawn_all_children(dbstate.servers)

    running = True
    while running:
        if reload_dbstate_config():
            kill_all_children(producers)
            spawn_all_children(dbstate.servers)

        if not handle_http_request(server):
            break

        find_new_servers()

        find_non_running_producers(dbstate.servers, producers, pending_producers)
        find_lockedup_producers(producers)
        pending_producers = create_pending_producers(pending_producers)
        dead_children = clean_up_dead_children(dead_children)

    kill_all_children(producers, signal.SIGKILL)
    os.unlink(pidfilepath)


